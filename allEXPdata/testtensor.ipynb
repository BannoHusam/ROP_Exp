{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, InputLayer\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from custom_pooling import RMSPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import LeakyReLU\n",
    "#from data import BALANCE_WEIGHTS\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#import os\n",
    "from datetime import datetime as dt\n",
    "from custom_loss import WeightedCategoricalCrossEntropy\n",
    "from tensorflow.keras import layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fractional_max_pool() got an unexpected keyword argument 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c1bb9a67cc67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mseed2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m tf.nn.fractional_max_pool(\n",
      "\u001b[0;31mTypeError\u001b[0m: fractional_max_pool() got an unexpected keyword argument 'input'"
     ]
    }
   ],
   "source": [
    "def get_experiment_id():\n",
    "    time_str = dt.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    experiment_id = 'base_{}'.format(time_str)\n",
    "    return experiment_id\n",
    "pooling_ratio=[0.57, 1.37, 2.30, 3.12] \n",
    "model = Sequential()\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "#model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "tf.nn.fractional_max_pool(\n",
    "    input=tf.shape(\n",
    "    input=(112,112,3),\n",
    "    name=None,\n",
    "    out_type=tf.int32\n",
    "),\n",
    "    value=32,\n",
    "    pooling_ratio=[0.57, 1.37, 2.30, 3.12],\n",
    "    pseudo_random=False,\n",
    "    overlapping=False,\n",
    "    deterministic=False,\n",
    "    seed=0,\n",
    "    seed2=0,\n",
    "    name=None\n",
    ")\n",
    "tf.nn.fractional_max_pool(\n",
    "    input=tf.shape(\n",
    "    input=(112,112,3),\n",
    "    name=None,\n",
    "    out_type=tf.int32\n",
    "),\n",
    "    value=64,\n",
    "    pooling_ratio=[0.57, 1.37, 2.30, 3.12],\n",
    "    pseudo_random=False,\n",
    "    overlapping=False,\n",
    "    deterministic=False,\n",
    "    seed=0,\n",
    "    seed2=0,\n",
    "    name=None\n",
    ")\n",
    "tf.nn.fractional_max_pool(\n",
    "    input=tf.shape(\n",
    "    input=(112,112,3),\n",
    "    name=None,\n",
    "    out_type=tf.int32\n",
    "),\n",
    "    value=128,\n",
    "    pooling_ratio=[0.57, 1.37, 2.30, 3.12],\n",
    "    pseudo_random=False,\n",
    "    overlapping=False,\n",
    "    deterministic=False,\n",
    "    seed=0,\n",
    "    seed2=0,\n",
    "    name=None\n",
    ")\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "# This is indeed relu in the original lasagne implementation, but softmax could be tried as well\n",
    "#Ir 0.01\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#adam = Adam(lr=0.0005, decay=1e-6)\n",
    "#rmsprop\n",
    "Mloss = WeightedCategoricalCrossEntropy({0: 0.4125861396437394, 1: 1.4918664786083686, 2: 0.690426457789382, 3: 3.9541433021806855, 4: 4.885604311008468})\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing Data Augmentation Configuration \n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "#        featurewise_center=True,\n",
    " #       samplewise_center=True,\n",
    "        shear_range=0,\n",
    "        zoom_range=0.0,#[1 / 1.15, 1.15],\n",
    "        horizontal_flip=False,#True,\n",
    "        rotation_range=0)#90,\n",
    "#        validation_split=0.2)\n",
    "\"\"\"\n",
    "For the test set only rescale should be made \n",
    "\"\"\"\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=r\"/media/husam/Data/project_2018/train_pre_sep\",\n",
    "    target_size=(112, 112),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=128,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42)\n",
    "#    subset=\"training\")\n",
    "#    save_to_dir=\"D:/Separating Images into subfolders/train_augmented\")\n",
    "\n",
    "\n",
    "#print(train_generator.class_indices)\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    directory=r\"/media/husam/Data/project_2018/val_set\",\n",
    "    target_size=(112, 112),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=128,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42)\n",
    "#    subset=\"validation\")\n",
    "#    save_to_dir=\"D:/Separating Images into subfolders/val_augmented\"\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=r\"/media/husam/Data/project_2018/test_pre_sep\",\n",
    "    target_size=(112, 112),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=1,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False,\n",
    "    seed=42)\n",
    "#    save_to_dir=\"D:/Separating Images into subfolders/test_augmented\")\n",
    "\n",
    "\n",
    "# Fitting/Training the model\n",
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "experiment_id = get_experiment_id()\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_acc', patience=50, min_delta=0.001, verbose=1),\n",
    "    ModelCheckpoint(experiment_id + \"-val_acc_checkpoint.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "    ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5, verbose=1, mode='auto', min_delta=0.001, cooldown=0, min_lr=1e-7)\n",
    "]\n",
    "\n",
    "class_weight_list = class_weight.compute_class_weight('balanced', np.unique(train_generator.classes), train_generator.classes)\n",
    "class_weights = dict(zip(np.unique(train_generator.classes), class_weight_list))\n",
    "\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    class_weight=class_weights,\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=25  #10 in original lasagne code it is 250\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluating the model \n",
    "#predicting output\n",
    "#reseting the test generator before calling predict_generator, in order not to get weird output\n",
    "test_generator.reset()\n",
    "#test_imgs, test_labels = next(test_generator)\n",
    "\n",
    "y_pred = model.predict_generator(test_generator, verbose=1)\n",
    "\n",
    "score = model.evaluate_generator(generator=test_generator, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#y_pred =\n",
    "\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1) #multiple categories\n",
    "# mapping the labels to its classes \n",
    "labels = (train_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in y_pred]\n",
    "\n",
    "y_true = (test_generator.classes)\n",
    "#confusion Matrix and Classification Report\n",
    "# generator.classes gives you the truth label y_true\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "print('Classification Report')\n",
    "target_names = ['0', '1', '2', '3', '4']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
