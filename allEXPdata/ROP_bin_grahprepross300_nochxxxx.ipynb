{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, InputLayer\n",
    "from keras.regularizers import L1L2, l2, l1\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from custom_pooling import RMSPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import LeakyReLU\n",
    "#from data import BALANCE_WEIGHTS\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#import os\n",
    "from datetime import datetime as dt\n",
    "from custom_loss import WeightedCategoricalCrossEntropy\n",
    "from VCM import VCM\n",
    "from QWK import QWK\n",
    "import tensorflow as tf\n",
    "from Fractional_MAXPOOL import FractionalPooling2D\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 296, 296, 16)      1216      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 292, 292, 16)      6416      \n",
      "_________________________________________________________________\n",
      "input_1 (InputLayer)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 290, 290, 16)      2320      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 288, 288, 16)      2320      \n",
      "_________________________________________________________________\n",
      "input_2 (InputLayer)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 284, 284, 16)      6416      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 280, 280, 16)      6416      \n",
      "_________________________________________________________________\n",
      "input_3 (InputLayer)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 276, 276, 16)      6416      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 272, 272, 16)      6416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 136, 136, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 134, 134, 32)      4640      \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 132, 132, 32)      9248      \n",
      "_________________________________________________________________\n",
      "input_4 (InputLayer)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 130, 130, 32)      9248      \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "input_5 (InputLayer)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 126, 126, 32)      9248      \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 124, 124, 32)      9248      \n",
      "_________________________________________________________________\n",
      "input_6 (InputLayer)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 122, 122, 32)      9248      \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 120, 120, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 60, 60, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 58, 58, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 56, 56, 64)        36928     \n",
      "_________________________________________________________________\n",
      "input_7 (InputLayer)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 54, 54, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 52, 52, 64)        36928     \n",
      "_________________________________________________________________\n",
      "input_8 (InputLayer)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 50, 50, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 48, 48, 64)        36928     \n",
      "_________________________________________________________________\n",
      "input_9 (InputLayer)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 22, 22, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 20, 20, 64)        36928     \n",
      "_________________________________________________________________\n",
      "input_10 (InputLayer)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 18, 18, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "input_11 (InputLayer)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 466,354\n",
      "Trainable params: 466,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 462 images belonging to 2 classes.\n",
      "Found 154 images belonging to 2 classes.\n",
      "Found 66 images belonging to 2 classes.\n",
      "Epoch 1/35\n",
      "7/7 [==============================] - 25s 4s/step - loss: 0.6972 - acc: 0.4731 - loss_no_weight_decay: 0.6972 - val_loss: 0.6912 - val_acc: 0.5234 - val_loss_no_weight_decay: 0.6912\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00001: val_kappa improved from -inf to 0.00000, saving model to base_2020-05-12-19-58-15-val_acc_checkpoint.hdf5\n",
      "Confusion matrix: \n",
      " [[42  1]\n",
      " [42  5]]\n",
      "Epoch 2/35\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.6952 - acc: 0.4955 - loss_no_weight_decay: 0.6952 - val_loss: 0.6892 - val_acc: 0.5000 - val_loss_no_weight_decay: 0.6892\n",
      "— val_kappa: -0.04444444444444451 \n",
      "\n",
      "Epoch 00002: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[61  0]\n",
      " [67  0]]\n",
      "Epoch 3/35\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.6918 - acc: 0.5251 - loss_no_weight_decay: 0.6918 - val_loss: 0.6863 - val_acc: 0.5222 - val_loss_no_weight_decay: 0.6863\n",
      "— val_kappa: 0.0888888888888889 \n",
      "\n",
      "Epoch 00003: val_kappa improved from 0.00000 to 0.08889, saving model to base_2020-05-12-19-58-15-val_acc_checkpoint.hdf5\n",
      "Confusion matrix: \n",
      " [[ 0 49]\n",
      " [ 0 41]]\n",
      "Epoch 4/35\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.6876 - acc: 0.5536 - loss_no_weight_decay: 0.6876 - val_loss: 0.6827 - val_acc: 0.5859 - val_loss_no_weight_decay: 0.6827\n",
      "— val_kappa: 0.09375 \n",
      "\n",
      "Epoch 00004: val_kappa improved from 0.08889 to 0.09375, saving model to base_2020-05-12-19-58-15-val_acc_checkpoint.hdf5\n",
      "Confusion matrix: \n",
      " [[ 8 38]\n",
      " [ 1 43]]\n",
      "Epoch 5/35\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.6924 - acc: 0.5889 - loss_no_weight_decay: 0.6924 - val_loss: 0.6838 - val_acc: 0.6778 - val_loss_no_weight_decay: 0.6838\n",
      "— val_kappa: 0.3111111111111111 \n",
      "\n",
      "Epoch 00005: val_kappa improved from 0.09375 to 0.31111, saving model to base_2020-05-12-19-58-15-val_acc_checkpoint.hdf5\n",
      "Confusion matrix: \n",
      " [[27 36]\n",
      " [ 8 57]]\n",
      "Epoch 6/35\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.6960 - acc: 0.5528 - loss_no_weight_decay: 0.6960 - val_loss: 0.6852 - val_acc: 0.5333 - val_loss_no_weight_decay: 0.6852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "— val_kappa: 0.06666666666666665 \n",
      "\n",
      "Epoch 00006: val_kappa did not improve from 0.31111\n",
      "Confusion matrix: \n",
      " [[43  0]\n",
      " [47  0]]\n",
      "Epoch 7/35\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.6918 - acc: 0.5069 - loss_no_weight_decay: 0.6918 - val_loss: 0.6944 - val_acc: 0.5078 - val_loss_no_weight_decay: 0.6944\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00007: val_kappa did not improve from 0.31111\n",
      "Confusion matrix: \n",
      " [[ 0 46]\n",
      " [ 0 44]]\n",
      "Epoch 8/35\n",
      "7/7 [==============================] - 11s 2s/step - loss: 0.6927 - acc: 0.5089 - loss_no_weight_decay: 0.6927 - val_loss: 0.6870 - val_acc: 0.5222 - val_loss_no_weight_decay: 0.6870\n",
      "— val_kappa: -0.022222222222222143 \n",
      "\n",
      "Epoch 00008: val_kappa did not improve from 0.31111\n",
      "Confusion matrix: \n",
      " [[62  0]\n",
      " [66  0]]\n",
      "Epoch 9/35\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.6939 - acc: 0.5019 - loss_no_weight_decay: 0.6939 - val_loss: 0.6899 - val_acc: 0.4889 - val_loss_no_weight_decay: 0.6899\n",
      "— val_kappa: 0.022222222222222254 \n",
      "\n",
      "Epoch 00009: val_kappa did not improve from 0.31111\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Confusion matrix: \n",
      " [[43  0]\n",
      " [47  0]]\n",
      "Epoch 10/35\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.6876 - acc: 0.5096 - loss_no_weight_decay: 0.6876 - val_loss: 0.6864 - val_acc: 0.5078 - val_loss_no_weight_decay: 0.6864\n",
      "— val_kappa: -0.03125 \n",
      "\n",
      "Epoch 00010: val_kappa did not improve from 0.31111\n",
      "Confusion matrix: \n",
      " [[48  0]\n",
      " [42  0]]\n",
      "Epoch 11/35\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.6856 - acc: 0.5317 - loss_no_weight_decay: 0.6856 - val_loss: 0.6919 - val_acc: 0.4556 - val_loss_no_weight_decay: 0.6919\n",
      "— val_kappa: 0.06666666666666665 \n",
      "\n",
      "Epoch 00011: val_kappa did not improve from 0.31111\n",
      "Confusion matrix: \n",
      " [[69  0]\n",
      " [59  0]]\n",
      "Epoch 12/35\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.6854 - acc: 0.4976 - loss_no_weight_decay: 0.6854 - val_loss: 0.6804 - val_acc: 0.5333 - val_loss_no_weight_decay: 0.6804\n",
      "— val_kappa: -0.04444444444444451 \n",
      "\n",
      "Epoch 00012: val_kappa did not improve from 0.31111\n",
      "Confusion matrix: \n",
      " [[45  0]\n",
      " [45  0]]\n",
      "Epoch 13/35\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.6863 - acc: 0.5022 - loss_no_weight_decay: 0.6863 - val_loss: 0.6825 - val_acc: 0.7344 - val_loss_no_weight_decay: 0.6825\n",
      "— val_kappa: 0.5 \n",
      "\n",
      "Epoch 00013: val_kappa improved from 0.31111 to 0.50000, saving model to base_2020-05-12-19-58-15-val_acc_checkpoint.hdf5\n",
      "Confusion matrix: \n",
      " [[36 11]\n",
      " [12 31]]\n",
      "Epoch 14/35\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.6799 - acc: 0.6821 - loss_no_weight_decay: 0.6799 - val_loss: 0.6799 - val_acc: 0.6333 - val_loss_no_weight_decay: 0.6799\n",
      "— val_kappa: 0.15555555555555556 \n",
      "\n",
      "Epoch 00014: val_kappa did not improve from 0.50000\n",
      "Confusion matrix: \n",
      " [[66  1]\n",
      " [50 11]]\n",
      "Epoch 15/35\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.6785 - acc: 0.5249 - loss_no_weight_decay: 0.6785 - val_loss: 0.6745 - val_acc: 0.6111 - val_loss_no_weight_decay: 0.6745\n",
      "— val_kappa: 0.33333333333333337 \n",
      "\n",
      "Epoch 00015: val_kappa did not improve from 0.50000\n",
      "Confusion matrix: \n",
      " [[44  0]\n",
      " [35 11]]\n",
      "Epoch 16/35\n",
      "7/7 [==============================] - 11s 2s/step - loss: 0.6751 - acc: 0.6997 - loss_no_weight_decay: 0.6751 - val_loss: 0.6703 - val_acc: 0.7578 - val_loss_no_weight_decay: 0.6703\n",
      "— val_kappa: 0.5625 \n",
      "\n",
      "Epoch 00016: val_kappa improved from 0.50000 to 0.56250, saving model to base_2020-05-12-19-58-15-val_acc_checkpoint.hdf5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Confusion matrix: \n",
      " [[26 19]\n",
      " [ 9 36]]\n",
      "Epoch 17/35\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.6709 - acc: 0.7039 - loss_no_weight_decay: 0.6709 - val_loss: 0.6692 - val_acc: 0.7111 - val_loss_no_weight_decay: 0.6692\n",
      "— val_kappa: 0.5777777777777777 \n",
      "\n",
      "Epoch 00017: val_kappa improved from 0.56250 to 0.57778, saving model to base_2020-05-12-19-58-15-val_acc_checkpoint.hdf5\n",
      "Confusion matrix: \n",
      " [[47 18]\n",
      " [11 52]]\n",
      "Epoch 18/35\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.6656 - acc: 0.7271 - loss_no_weight_decay: 0.6656 - val_loss: 0.6602 - val_acc: 0.8111 - val_loss_no_weight_decay: 0.6602\n",
      "— val_kappa: 0.5333333333333333 \n",
      "\n",
      "Epoch 00018: val_kappa did not improve from 0.57778\n",
      "Confusion matrix: \n",
      " [[32 15]\n",
      " [ 8 35]]\n",
      "Epoch 19/35\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.6579 - acc: 0.7344 - loss_no_weight_decay: 0.6579 - val_loss: 0.6533 - val_acc: 0.7344 - val_loss_no_weight_decay: 0.6533\n",
      "— val_kappa: 0.46875 \n",
      "\n",
      "Epoch 00019: val_kappa did not improve from 0.57778\n",
      "Confusion matrix: \n",
      " [[28 14]\n",
      " [18 30]]\n",
      "Epoch 20/35\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.6529 - acc: 0.6898 - loss_no_weight_decay: 0.6529 - val_loss: 0.7098 - val_acc: 0.4333 - val_loss_no_weight_decay: 0.7098\n",
      "— val_kappa: 0.0444444444444444 \n",
      "\n",
      "Epoch 00020: val_kappa did not improve from 0.57778\n",
      "Confusion matrix: \n",
      " [[ 0 67]\n",
      " [ 0 61]]\n",
      "Epoch 21/35\n",
      "7/7 [==============================] - 11s 2s/step - loss: 0.6582 - acc: 0.6362 - loss_no_weight_decay: 0.6582 - val_loss: 0.6372 - val_acc: 0.7333 - val_loss_no_weight_decay: 0.6372\n",
      "— val_kappa: 0.33333333333333337 \n",
      "\n",
      "Epoch 00021: val_kappa did not improve from 0.57778\n",
      "Confusion matrix: \n",
      " [[48  4]\n",
      " [27 11]]\n",
      "Epoch 22/35\n",
      "7/7 [==============================] - 11s 2s/step - loss: 0.6392 - acc: 0.7000 - loss_no_weight_decay: 0.6392 - val_loss: 0.6327 - val_acc: 0.7109 - val_loss_no_weight_decay: 0.6327\n",
      "— val_kappa: 0.375 \n",
      "\n",
      "Epoch 00022: val_kappa did not improve from 0.57778\n",
      "Confusion matrix: \n",
      " [[33  6]\n",
      " [23 28]]\n",
      "Epoch 23/35\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.6272 - acc: 0.6991 - loss_no_weight_decay: 0.6272 - val_loss: 0.6287 - val_acc: 0.6667 - val_loss_no_weight_decay: 0.6287\n",
      "— val_kappa: 0.4666666666666667 \n",
      "\n",
      "Epoch 00023: val_kappa did not improve from 0.57778\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Confusion matrix: \n",
      " [[40 23]\n",
      " [11 54]]\n",
      "Epoch 24/35\n",
      "7/7 [==============================] - 11s 2s/step - loss: 0.6200 - acc: 0.7076 - loss_no_weight_decay: 0.6200 - val_loss: 0.6031 - val_acc: 0.7444 - val_loss_no_weight_decay: 0.6031\n",
      "— val_kappa: 0.48888888888888893 \n",
      "\n",
      "Epoch 00024: val_kappa did not improve from 0.57778\n",
      "Confusion matrix: \n",
      " [[30 15]\n",
      " [ 9 36]]\n",
      "Epoch 25/35\n",
      "7/7 [==============================] - 11s 2s/step - loss: 0.6070 - acc: 0.7112 - loss_no_weight_decay: 0.6070 - val_loss: 0.5797 - val_acc: 0.7422 - val_loss_no_weight_decay: 0.5797\n",
      "— val_kappa: 0.40625 \n",
      "\n",
      "Epoch 00025: val_kappa did not improve from 0.57778\n",
      "Confusion matrix: \n",
      " [[35  9]\n",
      " [13 33]]\n",
      "Epoch 26/35\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.6021 - acc: 0.6934 - loss_no_weight_decay: 0.6021 - val_loss: 0.5849 - val_acc: 0.7000 - val_loss_no_weight_decay: 0.5849\n",
      "— val_kappa: 0.4444444444444444 \n",
      "\n",
      "Epoch 00026: val_kappa did not improve from 0.57778\n",
      "Confusion matrix: \n",
      " [[49 16]\n",
      " [16 47]]\n",
      "Epoch 27/35\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.5838 - acc: 0.7450 - loss_no_weight_decay: 0.5838 - val_loss: 0.5536 - val_acc: 0.7556 - val_loss_no_weight_decay: 0.5536\n",
      "— val_kappa: 0.4 \n",
      "\n",
      "Epoch 00027: val_kappa did not improve from 0.57778\n",
      "Confusion matrix: \n",
      " [[30 13]\n",
      " [ 8 39]]\n",
      "Epoch 28/35\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.5670 - acc: 0.7344 - loss_no_weight_decay: 0.5670 - val_loss: 0.5679 - val_acc: 0.7188 - val_loss_no_weight_decay: 0.5679\n",
      "— val_kappa: 0.453125 \n",
      "\n",
      "Epoch 00028: val_kappa did not improve from 0.57778\n",
      "Confusion matrix: \n",
      " [[28 19]\n",
      " [ 8 35]]\n",
      "Epoch 29/35\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.5622 - acc: 0.7501 - loss_no_weight_decay: 0.5622 - val_loss: 0.5064 - val_acc: 0.7889 - val_loss_no_weight_decay: 0.5064\n",
      "— val_kappa: 0.37777777777777777 \n",
      "\n",
      "Epoch 00029: val_kappa did not improve from 0.57778\n",
      "Confusion matrix: \n",
      " [[48 19]\n",
      " [18 43]]\n",
      "Epoch 30/35\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.5704 - acc: 0.7107 - loss_no_weight_decay: 0.5704 - val_loss: 0.5841 - val_acc: 0.7222 - val_loss_no_weight_decay: 0.5841\n",
      "— val_kappa: 0.5333333333333333 \n",
      "\n",
      "Epoch 00030: val_kappa did not improve from 0.57778\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      " [[23 21]\n",
      " [ 4 42]]\n",
      "Epoch 31/35\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.5428 - acc: 0.7210 - loss_no_weight_decay: 0.5428 - val_loss: 0.5314 - val_acc: 0.7578 - val_loss_no_weight_decay: 0.5314\n",
      "— val_kappa: 0.515625 \n",
      "\n",
      "Epoch 00031: val_kappa did not improve from 0.57778\n",
      "Confusion matrix: \n",
      " [[41  5]\n",
      " [10 34]]\n",
      "Epoch 32/35\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.5429 - acc: 0.7316 - loss_no_weight_decay: 0.5429 - val_loss: 0.5030 - val_acc: 0.7556 - val_loss_no_weight_decay: 0.5030\n",
      "— val_kappa: 0.48888888888888893 \n",
      "\n",
      "Epoch 00032: val_kappa did not improve from 0.57778\n",
      "Confusion matrix: \n",
      " [[52 13]\n",
      " [20 43]]\n",
      "Epoch 33/35\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.5507 - acc: 0.7089 - loss_no_weight_decay: 0.5507 - val_loss: 0.4957 - val_acc: 0.7778 - val_loss_no_weight_decay: 0.4957\n",
      "— val_kappa: 0.37777777777777777 \n",
      "\n",
      "Epoch 00033: val_kappa did not improve from 0.57778\n",
      "Confusion matrix: \n",
      " [[37  9]\n",
      " [ 8 36]]\n",
      "Epoch 34/35\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.5075 - acc: 0.7612 - loss_no_weight_decay: 0.5075 - val_loss: 0.4919 - val_acc: 0.7734 - val_loss_no_weight_decay: 0.4919\n",
      "— val_kappa: 0.46875 \n",
      "\n",
      "Epoch 00034: val_kappa did not improve from 0.57778\n",
      "Confusion matrix: \n",
      " [[45  6]\n",
      " [12 27]]\n",
      "Epoch 35/35\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.5405 - acc: 0.7500 - loss_no_weight_decay: 0.5405 - val_loss: 0.4780 - val_acc: 0.7111 - val_loss_no_weight_decay: 0.4780\n",
      "— val_kappa: 0.5111111111111111 \n",
      "\n",
      "Epoch 00035: val_kappa did not improve from 0.57778\n",
      "Confusion matrix: \n",
      " [[51 18]\n",
      " [15 44]]\n",
      "66/66 [==============================] - 1s 14ms/step\n",
      "66/66 [==============================] - 1s 10ms/step\n",
      "Test loss: 0.625305079826803\n",
      "Test accuracy: 0.6515151515151515\n",
      "Confusion Matrix\n",
      "[[23 10]\n",
      " [13 20]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.70      0.67        33\n",
      "           1       0.67      0.61      0.63        33\n",
      "\n",
      "    accuracy                           0.65        66\n",
      "   macro avg       0.65      0.65      0.65        66\n",
      "weighted avg       0.65      0.65      0.65        66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_experiment_id():\n",
    "    time_str = dt.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    experiment_id = 'base_{}'.format(time_str)\n",
    "    return experiment_id\n",
    "p_ratio=[1.0, 1.44, 1.73, 1.0]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Conv2D(16, (5, 5), activation='tanh', input_shape=(300, 300, 3)))\n",
    "model.add(Conv2D(16, (5, 5), activation='tanh'))\n",
    "model.add(InputLayer(input_tensor=tf.nn.fractional_max_pool(model.layers[1].output, p_ratio)[0]))\n",
    "#model.add(Dense(8,activation='tanh', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "#model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='tanh'))\n",
    "model.add(Conv2D(16, (3, 3), activation='tanh'))\n",
    "model.add(InputLayer(input_tensor=tf.nn.fractional_max_pool(model.layers[1].output, p_ratio)[0]))\n",
    "#model.add(Dense(8,activation='tanh', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "\n",
    "#model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "\n",
    "model.add(Conv2D(16, (5, 5), activation='tanh'))\n",
    "model.add(Conv2D(16, (5, 5), activation='tanh'))\n",
    "model.add(InputLayer(input_tensor=tf.nn.fractional_max_pool(model.layers[1].output, p_ratio)[0]))\n",
    "#model.add(Dense(8,activation='tanh', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "#model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Conv2D(16, (5, 5), activation='tanh'))\n",
    "model.add(Conv2D(16, (5, 5), activation='tanh'))\n",
    "#model.add(InputLayer(input_tensor=tf.nn.fractional_max_pool(model.layers[1].output, p_ratio)[0]))\n",
    "#model.add(Dense(8,activation='tanh', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "#model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='tanh'))\n",
    "model.add(Conv2D(32, (3, 3), activation='tanh'))\n",
    "model.add(InputLayer(input_tensor=tf.nn.fractional_max_pool(model.layers[1].output, p_ratio)[0]))\n",
    "#model.add(Dense(8,activation='tanh', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "#model.add(LeakyReLU(alpha=0.01))          \n",
    "model.add(Conv2D(32, (3, 3), activation='tanh'))\n",
    "model.add(Conv2D(32, (3, 3), activation='tanh'))\n",
    "model.add(InputLayer(input_tensor=tf.nn.fractional_max_pool(model.layers[1].output, p_ratio)[0]))\n",
    "#model.add(Dense(8,activation='tanh', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "#model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='tanh'))\n",
    "model.add(Conv2D(32, (3, 3), activation='tanh'))\n",
    "model.add(InputLayer(input_tensor=tf.nn.fractional_max_pool(model.layers[1].output, p_ratio)[0]))\n",
    "#model.add(Dense(8,activation='tanh', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='tanh'))\n",
    "model.add(Conv2D(32, (3, 3), activation='tanh'))\n",
    "#model.add(InputLayer(input_tensor=tf.nn.fractional_max_pool(model.layers[1].output, p_ratio)[0]))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='tanh'))\n",
    "model.add(Conv2D(64, (3, 3), activation='tanh'))\n",
    "model.add(InputLayer(input_tensor=tf.nn.fractional_max_pool(model.layers[1].output, p_ratio)[0]))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='tanh'))\n",
    "model.add(Conv2D(64, (3, 3), activation='tanh'))\n",
    "model.add(InputLayer(input_tensor=tf.nn.fractional_max_pool(model.layers[1].output, p_ratio)[0]))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='tanh'))\n",
    "model.add(Conv2D(64, (3, 3), activation='tanh'))\n",
    "model.add(InputLayer(input_tensor=tf.nn.fractional_max_pool(model.layers[1].output, p_ratio)[0]))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "#model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Conv2D(64, (3, 3), activation='tanh'))\n",
    "model.add(Conv2D(64, (3, 3), activation='tanh'))\n",
    "model.add(InputLayer(input_tensor=tf.nn.fractional_max_pool(model.layers[1].output, p_ratio)[0]))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='tanh'))\n",
    "model.add(Conv2D(64, (3, 3), activation='tanh'))\n",
    "model.add(InputLayer(input_tensor=tf.nn.fractional_max_pool(model.layers[1].output, p_ratio)[0]))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "#chnaged from 10 to 5 because an error \n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.count_params()\n",
    "#adam = Adam(lr=0.0005, decay=1e-6)\n",
    "#rmsprop\n",
    "#Mloss = WeightedCategoricalCrossEntropy({0: 0.4125861396437394, 1: 1.4918664786083686, 2: 0.690426457789382, 3: 3.9541433021806855, 4: 4.885604311008468})\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "model.summary()\n",
    "loss_no_weight_decay = model.total_loss - sum(model.losses)\n",
    "model.metrics_tensors.append(loss_no_weight_decay)\n",
    "model.metrics_names.append('loss_no_weight_decay')\n",
    "#############################################################################################\n",
    "#############################################################################################\n",
    "#############################################################################################\n",
    "#############################################################################################\n",
    "#Preparing Data Augmentation Configuration \n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "#        featurewise_center=True,\n",
    " #       samplewise_center=True,\n",
    "        shear_range=0,\n",
    "        zoom_range=0.0,#[1 / 1.15, 1.15],\n",
    "        horizontal_flip=False,#True,\n",
    "        rotation_range=0)#90,\n",
    "#        validation_split=0.2)\n",
    "\"\"\"\n",
    "For the test set only rescale should be made \n",
    "\"\"\"\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=r\"/media/husam/Data/ROP_EXp/grahprepross/bin_bal/train\",\n",
    "    target_size=(300, 300),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=64,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42)\n",
    "#    subset=\"training\")\n",
    "#    save_to_dir=\"D:/Separating Images into subfolders/train_augmented\")\n",
    "\n",
    "\n",
    "#print(train_generator.class_indices)\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    directory=r\"/media/husam/Data/ROP_EXp/grahprepross/bin_bal/vald\",\n",
    "    target_size=(300, 300),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=64,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42)\n",
    "#    subset=\"validation\")\n",
    "#    save_to_dir=\"D:/Separating Images into subfolders/val_augmented\"\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=r\"/media/husam/Data/ROP_EXp/grahprepross/bin_bal/test\",\n",
    "    target_size=(300, 300),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=1,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False,\n",
    "    seed=42)\n",
    "#    save_to_dir=\"D:/Separating Images into subfolders/test_augmented\")\n",
    "\n",
    "\n",
    "# Fitting/Training the model\n",
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "experiment_id = get_experiment_id()\n",
    "callbacks = [\n",
    "    QWK(valid_generator, STEP_SIZE_VALID),\n",
    "    EarlyStopping(monitor='val_acc', patience=50, min_delta=0.001, verbose=1),\n",
    "    ModelCheckpoint(experiment_id + \"-val_acc_checkpoint.hdf5\", monitor='val_kappa', verbose=1,save_best_only=True, mode='max'),\n",
    "    ReduceLROnPlateau(monitor='val_kappa', factor=0.5, patience=7, verbose=1, mode='auto', min_delta=0.001, cooldown=0, min_lr=1e-7),\n",
    "    VCM(valid_generator,STEP_SIZE_VALID)\n",
    "    \n",
    "]\n",
    "\n",
    "class_weight_list = class_weight.compute_class_weight('balanced', np.unique(train_generator.classes), train_generator.classes)\n",
    "class_weights = dict(zip(np.unique(train_generator.classes), class_weight_list))\n",
    "\n",
    "history=model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    #class_weight=class_weights,\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=35  \n",
    ")\n",
    "\n",
    "\n",
    "# Evaluating the model \n",
    "#predicting output\n",
    "#reseting the test generator before calling predict_generator, in order not to get weird output\n",
    "test_generator.reset()\n",
    "#test_imgs, test_labels = next(test_generator)\n",
    "\n",
    "y_pred = model.predict_generator(test_generator, verbose=1,steps=len(test_generator))\n",
    "\n",
    "score = model.evaluate_generator(generator=test_generator, verbose=1,steps=len(test_generator))\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#y_pred =\n",
    "\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1) #multiple categories\n",
    "# mapping the labels to its classes \n",
    "labels = (train_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in y_pred]\n",
    "\n",
    "y_true = (test_generator.classes)\n",
    "#confusion Matrix and Classification Report\n",
    "# generator.classes gives you the truth label y_true\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "print('Classification Report')\n",
    "target_names = ['0', '1']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
