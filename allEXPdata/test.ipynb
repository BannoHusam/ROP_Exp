{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 124, 124, 32)      2432      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 124, 124, 32)      128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 124, 124, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 122, 122, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 122, 122, 32)      128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 122, 122, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 60, 60, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 56, 56, 64)        51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 54, 54, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 54, 54, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 52, 52, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 52, 52, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 52, 52, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 23, 23, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 23, 23, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 23, 23, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 21, 21, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 21, 21, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 19, 19, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 19, 19, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 19, 19, 128)       0         \n",
      "_________________________________________________________________\n",
      "rms_pooling2d_2 (RMSPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6, 6, 1024)        132096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 6, 6, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 6, 6, 1024)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 3, 3, 1024)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              9438208   \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 10,087,905\n",
      "Trainable params: 10,082,529\n",
      "Non-trainable params: 5,376\n",
      "_________________________________________________________________\n",
      "Found 15175 validated image filenames.\n",
      "Found 5056 validated image filenames.\n",
      "Found 5056 images belonging to 5 classes.\n",
      "Epoch 1/3\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
      "237/237 [==============================] - 261s 1s/step - loss: 1.9926 - mean_absolute_error: 0.6284 - acc: 0.5821 - val_loss: 1.8386 - val_mean_absolute_error: 0.7182 - val_acc: 0.6185\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.61847, saving model to base_2019-03-27-16-20-21-val_acc_checkpoint.hdf5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [79, 5056]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b3cd227cfa2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    252\u001b[0m                     \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                     epochs=3)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/VCM.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0my_pred_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Confusion matrix:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \"\"\"\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [79, 5056]"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D,BatchNormalization,Dense, Dropout, Flatten, LeakyReLU\n",
    "from custom_pooling import RMSPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report, cohen_kappa_score\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,LearningRateScheduler, TensorBoard\n",
    "from datetime import datetime as dt\n",
    "from VCM import VCM \n",
    "from keras.initializers import Constant,Orthogonal\n",
    "from keras import regularizers\n",
    "#from Kappa_Skl import kappa\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "def get_experiment_id():\n",
    "    time_str = dt.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    experiment_id = 'base_{}'.format(time_str)\n",
    "    return experiment_id\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch == 150:\n",
    "        K.set_value(model.optimizer.lr, 0.0003)\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    " \n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "    plt.savefig('CM.png')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (5, 5), activation='linear', input_shape=(128, 128, 3), data_format=\"channels_last\", kernel_initializer=Orthogonal(gain=1.0),\n",
    "                 bias_initializer=Constant(value=0.05),kernel_regularizer=regularizers.l2(0.0005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='linear', kernel_initializer=Orthogonal(gain=1.0),\n",
    "                 bias_initializer=Constant(value=0.05),kernel_regularizer=regularizers.l2(0.0005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=3, strides=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (5, 5), activation='linear', kernel_initializer=Orthogonal(gain=1.0),\n",
    "                 bias_initializer=Constant(value=0.05),kernel_regularizer=regularizers.l2(0.0005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='linear', kernel_initializer=Orthogonal(gain=1.0),\n",
    "                 bias_initializer=Constant(value=0.05),kernel_regularizer=regularizers.l2(0.0005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='linear', kernel_initializer=Orthogonal(gain=1.0),\n",
    "                 bias_initializer=Constant(value=0.05),kernel_regularizer=regularizers.l2(0.0005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=3, strides=(2, 2)))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='linear', kernel_initializer=Orthogonal(gain=1.0),\n",
    "                 bias_initializer=Constant(value=0.05),kernel_regularizer=regularizers.l2(0.0005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='linear', kernel_initializer=Orthogonal(gain=1.0),\n",
    "                 bias_initializer=Constant(value=0.05),kernel_regularizer=regularizers.l2(0.0005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='linear', kernel_initializer=Orthogonal(gain=1.0),\n",
    "                 bias_initializer=Constant(value=0.05),kernel_regularizer=regularizers.l2(0.0005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(RMSPooling2D(pool_size=3,strides=(3, 3)))\n",
    "\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(1024, activation='linear', kernel_initializer=Orthogonal(gain=1.0),\n",
    "                 bias_initializer=Constant(value=0.05),kernel_regularizer=regularizers.l2(0.0005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1024, activation='linear', kernel_initializer=Orthogonal(gain=1.0),\n",
    "                 bias_initializer=Constant(value=0.05),kernel_regularizer=regularizers.l2(0.0005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "model.add(Dense(1, activation='relu', kernel_initializer=Orthogonal(gain=1.0),\n",
    "                 bias_initializer=Constant(value=0.05),kernel_regularizer=regularizers.l2(0.0005)))\n",
    "\n",
    "sgd = SGD(lr=0.003, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss='mse', optimizer=sgd, metrics=['mae', 'acc'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "experiment_id = get_experiment_id()\n",
    "\n",
    "def append_ext(fn):\n",
    "    return fn+\".jpeg\"\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "traindf=pd.read_csv((\"/media/husam/Data/project_2018/train_pre_sep/tran_lab2.csv\"), dtype={'image': str, 'level': float})\n",
    "\n",
    "valdf=pd.read_csv((\"/media/husam/Data/project_2018/val_set/val_lab3.csv\"), dtype={'image': str, 'level': float})\n",
    "\n",
    "traindf[\"image\"]=traindf[\"image\"].apply(append_ext)\n",
    "valdf[\"image\"]=valdf[\"image\"].apply(append_ext)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "dataframe=traindf,\n",
    "directory=r\"/media/husam/Data/project_2018/train_pre_sep2\",\n",
    "x_col=\"image\",\n",
    "y_col=\"level\",\n",
    "has_ext=True,                                      \n",
    "batch_size=64,\n",
    "seed=42,\n",
    "shuffle=True,\n",
    "class_mode=\"other\",\n",
    "target_size=(128,128))\n",
    "\n",
    "\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_dataframe(\n",
    "dataframe=valdf,\n",
    "directory=r\"/media/husam/Data/project_2018/val_set2\",\n",
    "x_col=\"image\",\n",
    "y_col=\"level\",\n",
    "has_ext=True,                                      \n",
    "batch_size=64,\n",
    "seed=42,\n",
    "shuffle=True,\n",
    "class_mode=\"other\",\n",
    "target_size=(128,128))\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=r\"/media/husam/Data/project_2018/test_pre_sep\",\n",
    "    target_size=(128, 128),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=1,\n",
    "    class_mode=\"sparse\",\n",
    "    shuffle=False,\n",
    "    seed=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class_weights1 = class_weight.compute_class_weight(\n",
    "               'balanced',\n",
    "                np.unique(traindf.level), \n",
    "                traindf.level)\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_acc', patience=100, min_delta=0.001, verbose=1, restore_best_weights=True),\n",
    "    ModelCheckpoint(experiment_id + \"-val_acc_checkpoint.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "    VCM(valid_generator, STEP_SIZE_VALID),\n",
    "    LearningRateScheduler(scheduler, verbose=1),\n",
    "    TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "history=model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    class_weight=class_weights1,\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=3)\n",
    "\n",
    "\n",
    "y_pred = model.predict_generator(test_generator, verbose=1)\n",
    "\n",
    "test_generator.reset()\n",
    "score = model.evaluate_generator(generator=test_generator, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test mae:', score[1])\n",
    "print('Test accuracy:', score[2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predicted_class_indices = np.round(y_pred).astype(int)\n",
    "y_true = (test_generator.classes)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "cm=confusion_matrix(y_true, predicted_class_indices)\n",
    "print(cm)\n",
    "\n",
    "plot_confusion_matrix(cm,target_names=['0', '1', '2', '3', '4'],title='Confusion matrix',normalize=False)\n",
    "plot_confusion_matrix(cm,target_names=['0', '1', '2', '3', '4'],title='Confusion matrix Normalized',normalize=True)\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(y_true, predicted_class_indices, target_names=['0', '1', '2', '3', '4']))\n",
    "\n",
    "\n",
    "#print('kappa_score')\n",
    "#print(cohen_kappa_score(y_true, predicted_class_indices))\n",
    "#\n",
    "#print('Quadratic_wieghted_kappa')\n",
    "#print(kappa(y_true, predicted_class_indices, weights='quadratic' ))\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig('model accuracy.png')\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig('model loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
