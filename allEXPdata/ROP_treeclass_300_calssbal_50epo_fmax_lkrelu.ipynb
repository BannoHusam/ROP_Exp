{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, InputLayer\n",
    "from keras.regularizers import L1L2, l2, l1\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from custom_pooling import RMSPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import LeakyReLU\n",
    "#from data import BALANCE_WEIGHTS\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#import os\n",
    "from datetime import datetime as dt\n",
    "from custom_loss import WeightedCategoricalCrossEntropy\n",
    "from VCM import VCM\n",
    "from QWK import QWK\n",
    "import keras.backend as K\n",
    "from Fractional_MAXPOOL import FractionalPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 296, 296, 16)      1216      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 296, 296, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 292, 292, 16)      6416      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 292, 292, 16)      0         \n",
      "_________________________________________________________________\n",
      "fractional_pooling2d_1 (Frac (None, 162, 162, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 160, 160, 16)      2320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 160, 160, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 158, 158, 16)      2320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 158, 158, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 154, 154, 16)      6416      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 154, 154, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 150, 150, 16)      6416      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 150, 150, 16)      0         \n",
      "_________________________________________________________________\n",
      "fractional_pooling2d_2 (Frac (None, 83, 83, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 79, 79, 16)        6416      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 79, 79, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 75, 75, 16)        6416      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 75, 75, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 73, 73, 32)        4640      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 73, 73, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 71, 71, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 71, 71, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 69, 69, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 69, 69, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 67, 67, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 67, 67, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 65, 65, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 65, 65, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 63, 63, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 63, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "fractional_pooling2d_3 (Frac (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 35, 35, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 35, 35, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 33, 33, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 33, 33, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 31, 31, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 29, 29, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 29, 29, 64)        0         \n",
      "_________________________________________________________________\n",
      "fractional_pooling2d_4 (Frac (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 15, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 9, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "fractional_pooling2d_5 (Frac (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 4803      \n",
      "=================================================================\n",
      "Total params: 315,251\n",
      "Trainable params: 315,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 588 images belonging to 3 classes.\n",
      "Found 116 images belonging to 3 classes.\n",
      "Found 176 images belonging to 3 classes.\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 29s 3s/step - loss: 1.0912 - acc: 0.4539 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4444 - loss_no_weight_decay: 1.0912 - val_loss: 1.0983 - val_acc: 0.5781 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4444 - val_loss_no_weight_decay: 1.0983\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00001: val_kappa improved from -inf to 0.00000, saving model to base_2020-05-09-15-06-43-val_kapa_checkpoint.hdf5\n",
      "Confusion matrix: \n",
      " [[26  0  0]\n",
      " [ 5  0  0]\n",
      " [21  0  0]]\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 20s 2s/step - loss: 1.0957 - acc: 0.5256 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4444 - loss_no_weight_decay: 1.0957 - val_loss: 1.0988 - val_acc: 0.4038 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4445 - val_loss_no_weight_decay: 1.0988\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00002: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[27  0  0]\n",
      " [ 7  0  0]\n",
      " [18  0  0]]\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 19s 2s/step - loss: 1.1277 - acc: 0.4624 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4444 - loss_no_weight_decay: 1.1277 - val_loss: 1.0985 - val_acc: 0.5000 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4444 - val_loss_no_weight_decay: 1.0985\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00003: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[22  0  0]\n",
      " [ 6  0  0]\n",
      " [24  0  0]]\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0666 - acc: 0.5160 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4444 - loss_no_weight_decay: 1.0666 - val_loss: 1.0982 - val_acc: 0.5192 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0982\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00004: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[23  0  0]\n",
      " [ 6  0  0]\n",
      " [23  0  0]]\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0973 - acc: 0.4356 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.0973 - val_loss: 1.0980 - val_acc: 0.3281 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0980\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00005: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[28  0  0]\n",
      " [ 5  0  0]\n",
      " [19  0  0]]\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0844 - acc: 0.3414 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.0844 - val_loss: 1.0975 - val_acc: 0.4423 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4442 - val_loss_no_weight_decay: 1.0975\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00006: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[29  0  0]\n",
      " [ 5  0  0]\n",
      " [18  0  0]]\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0982 - acc: 0.3667 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.0982 - val_loss: 1.0973 - val_acc: 0.4375 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4441 - val_loss_no_weight_decay: 1.0973\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00007: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[29  0  0]\n",
      " [ 4  0  0]\n",
      " [19  0  0]]\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.1135 - acc: 0.3963 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4442 - loss_no_weight_decay: 1.1135 - val_loss: 1.0979 - val_acc: 0.3462 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0979\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00008: val_kappa did not improve from 0.00000\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Confusion matrix: \n",
      " [[28  0  0]\n",
      " [ 7  0  0]\n",
      " [17  0  0]]\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 21s 2s/step - loss: 1.0951 - acc: 0.3611 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.0951 - val_loss: 1.0980 - val_acc: 0.3750 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0980\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00009: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[26  0  0]\n",
      " [ 6  0  0]\n",
      " [20  0  0]]\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.1102 - acc: 0.3587 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.1102 - val_loss: 1.0983 - val_acc: 0.3269 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4444 - val_loss_no_weight_decay: 1.0983\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00010: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[29  0  0]\n",
      " [ 4  0  0]\n",
      " [19  0  0]]\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0775 - acc: 0.3690 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.0775 - val_loss: 1.0983 - val_acc: 0.3438 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4444 - val_loss_no_weight_decay: 1.0983\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00011: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[26  0  0]\n",
      " [ 4  0  0]\n",
      " [22  0  0]]\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.1126 - acc: 0.3808 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.1126 - val_loss: 1.0983 - val_acc: 0.3654 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4444 - val_loss_no_weight_decay: 1.0983\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00012: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[31  0  0]\n",
      " [ 4  0  0]\n",
      " [17  0  0]]\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 20s 2s/step - loss: 1.0865 - acc: 0.3655 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4444 - loss_no_weight_decay: 1.0865 - val_loss: 1.0979 - val_acc: 0.4219 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0979\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00013: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[28  0  0]\n",
      " [ 5  0  0]\n",
      " [19  0  0]]\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 21s 2s/step - loss: 1.1651 - acc: 0.3681 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4444 - loss_no_weight_decay: 1.1651 - val_loss: 1.0991 - val_acc: 0.3269 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4446 - val_loss_no_weight_decay: 1.0991\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00014: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[27  0  0]\n",
      " [ 6  0  0]\n",
      " [19  0  0]]\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 18s 2s/step - loss: 1.0622 - acc: 0.3868 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4444 - loss_no_weight_decay: 1.0622 - val_loss: 1.0985 - val_acc: 0.4062 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4444 - val_loss_no_weight_decay: 1.0985\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00015: val_kappa did not improve from 0.00000\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Confusion matrix: \n",
      " [[31  0  0]\n",
      " [ 5  0  0]\n",
      " [16  0  0]]\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 21s 2s/step - loss: 1.0937 - acc: 0.3698 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4444 - loss_no_weight_decay: 1.0937 - val_loss: 1.0986 - val_acc: 0.3654 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4445 - val_loss_no_weight_decay: 1.0986\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00016: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[24  0  0]\n",
      " [ 9  0  0]\n",
      " [19  0  0]]\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 18s 2s/step - loss: 1.0881 - acc: 0.3590 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4445 - loss_no_weight_decay: 1.0881 - val_loss: 1.0985 - val_acc: 0.3750 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4444 - val_loss_no_weight_decay: 1.0985\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00017: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[24  0  0]\n",
      " [ 6  0  0]\n",
      " [22  0  0]]\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 21s 2s/step - loss: 1.0793 - acc: 0.3698 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4444 - loss_no_weight_decay: 1.0793 - val_loss: 1.0985 - val_acc: 0.3654 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4444 - val_loss_no_weight_decay: 1.0985\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00018: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[32  0  0]\n",
      " [ 3  0  0]\n",
      " [17  0  0]]\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.1335 - acc: 0.3793 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4444 - loss_no_weight_decay: 1.1335 - val_loss: 1.0982 - val_acc: 0.4062 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0982\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00019: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[24  0  0]\n",
      " [ 6  0  0]\n",
      " [22  0  0]]\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0784 - acc: 0.3705 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4444 - loss_no_weight_decay: 1.0784 - val_loss: 1.0989 - val_acc: 0.3269 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4445 - val_loss_no_weight_decay: 1.0989\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00020: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[24  0  0]\n",
      " [ 6  0  0]\n",
      " [22  0  0]]\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 19s 2s/step - loss: 1.0824 - acc: 0.3720 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4444 - loss_no_weight_decay: 1.0824 - val_loss: 1.0989 - val_acc: 0.3125 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4445 - val_loss_no_weight_decay: 1.0989\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00021: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[21  0  0]\n",
      " [ 9  0  0]\n",
      " [22  0  0]]\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0991 - acc: 0.3828 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4444 - loss_no_weight_decay: 1.0991 - val_loss: 1.0979 - val_acc: 0.4231 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0979\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00022: val_kappa did not improve from 0.00000\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Confusion matrix: \n",
      " [[27  0  0]\n",
      " [ 9  0  0]\n",
      " [16  0  0]]\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0780 - acc: 0.3692 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4444 - loss_no_weight_decay: 1.0780 - val_loss: 1.0979 - val_acc: 0.4062 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0979\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00023: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[26  0  0]\n",
      " [ 6  0  0]\n",
      " [20  0  0]]\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0653 - acc: 0.3495 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4444 - loss_no_weight_decay: 1.0653 - val_loss: 1.0987 - val_acc: 0.3077 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4445 - val_loss_no_weight_decay: 1.0987\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00024: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[23  0  0]\n",
      " [ 7  0  0]\n",
      " [22  0  0]]\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0972 - acc: 0.3895 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.0972 - val_loss: 1.0974 - val_acc: 0.4531 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4442 - val_loss_no_weight_decay: 1.0974\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00025: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[25  0  0]\n",
      " [ 6  0  0]\n",
      " [21  0  0]]\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 22s 2s/step - loss: 1.1017 - acc: 0.3733 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.1017 - val_loss: 1.0977 - val_acc: 0.4231 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4442 - val_loss_no_weight_decay: 1.0977\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00026: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[26  0  0]\n",
      " [ 6  0  0]\n",
      " [20  0  0]]\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0880 - acc: 0.3677 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.0880 - val_loss: 1.0984 - val_acc: 0.3281 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4444 - val_loss_no_weight_decay: 1.0984\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00027: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[27  0  0]\n",
      " [ 7  0  0]\n",
      " [18  0  0]]\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0918 - acc: 0.3394 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4444 - loss_no_weight_decay: 1.0918 - val_loss: 1.0979 - val_acc: 0.3846 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0979\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00028: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[25  0  0]\n",
      " [ 8  0  0]\n",
      " [19  0  0]]\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.1063 - acc: 0.3417 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4444 - loss_no_weight_decay: 1.1063 - val_loss: 1.0980 - val_acc: 0.3750 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0980\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00029: val_kappa did not improve from 0.00000\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Confusion matrix: \n",
      " [[27  0  0]\n",
      " [ 5  0  0]\n",
      " [20  0  0]]\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0837 - acc: 0.3793 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.0837 - val_loss: 1.0981 - val_acc: 0.3654 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0981\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00030: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[23  0  0]\n",
      " [ 5  0  0]\n",
      " [24  0  0]]\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0636 - acc: 0.3554 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.0636 - val_loss: 1.0978 - val_acc: 0.3750 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0978\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00031: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[27  0  0]\n",
      " [ 2  0  0]\n",
      " [23  0  0]]\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 22s 2s/step - loss: 1.1103 - acc: 0.3646 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.1103 - val_loss: 1.0973 - val_acc: 0.4615 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4441 - val_loss_no_weight_decay: 1.0973\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00032: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[24  0  0]\n",
      " [ 7  0  0]\n",
      " [21  0  0]]\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0782 - acc: 0.3924 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.0782 - val_loss: 1.0978 - val_acc: 0.3750 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0978\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00033: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[27  0  0]\n",
      " [ 7  0  0]\n",
      " [18  0  0]]\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0988 - acc: 0.3530 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.0988 - val_loss: 1.0977 - val_acc: 0.4038 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4442 - val_loss_no_weight_decay: 1.0977\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00034: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[23  0  0]\n",
      " [ 6  0  0]\n",
      " [23  0  0]]\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 21s 2s/step - loss: 1.0761 - acc: 0.3715 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.0761 - val_loss: 1.0974 - val_acc: 0.4375 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4442 - val_loss_no_weight_decay: 1.0974\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00035: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[31  0  0]\n",
      " [ 3  0  0]\n",
      " [18  0  0]]\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 17s 2s/step - loss: 1.0657 - acc: 0.3523 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.0657 - val_loss: 1.0974 - val_acc: 0.4423 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4442 - val_loss_no_weight_decay: 1.0974\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00036: val_kappa did not improve from 0.00000\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Confusion matrix: \n",
      " [[29  0  0]\n",
      " [ 8  0  0]\n",
      " [15  0  0]]\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 21s 2s/step - loss: 1.1031 - acc: 0.3785 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.1031 - val_loss: 1.0977 - val_acc: 0.3750 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4442 - val_loss_no_weight_decay: 1.0977\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00037: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[23  0  0]\n",
      " [ 6  0  0]\n",
      " [23  0  0]]\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0835 - acc: 0.3499 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.0835 - val_loss: 1.0984 - val_acc: 0.2885 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4444 - val_loss_no_weight_decay: 1.0984\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00038: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[27  0  0]\n",
      " [ 7  0  0]\n",
      " [18  0  0]]\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 19s 2s/step - loss: 1.0998 - acc: 0.3589 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.0998 - val_loss: 1.0979 - val_acc: 0.3438 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0979\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00039: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[29  0  0]\n",
      " [ 7  0  0]\n",
      " [16  0  0]]\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.1004 - acc: 0.3878 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4442 - loss_no_weight_decay: 1.1004 - val_loss: 1.0980 - val_acc: 0.3462 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0980\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00040: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[31  0  0]\n",
      " [ 5  0  0]\n",
      " [16  0  0]]\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 20s 2s/step - loss: 1.0889 - acc: 0.3570 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.0889 - val_loss: 1.0973 - val_acc: 0.4531 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4441 - val_loss_no_weight_decay: 1.0973\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00041: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[28  0  0]\n",
      " [ 4  0  0]\n",
      " [20  0  0]]\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 21s 2s/step - loss: 1.0928 - acc: 0.3663 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.0928 - val_loss: 1.0981 - val_acc: 0.3077 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0981\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00042: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[24  0  0]\n",
      " [ 6  0  0]\n",
      " [22  0  0]]\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 18s 2s/step - loss: 1.0809 - acc: 0.3623 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.0809 - val_loss: 1.0981 - val_acc: 0.3281 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0981\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00043: val_kappa did not improve from 0.00000\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Confusion matrix: \n",
      " [[29  0  0]\n",
      " [ 3  0  0]\n",
      " [20  0  0]]\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 20s 2s/step - loss: 1.1193 - acc: 0.4174 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4442 - loss_no_weight_decay: 1.1193 - val_loss: 1.0974 - val_acc: 0.4231 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4442 - val_loss_no_weight_decay: 1.0974\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00044: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[25  0  0]\n",
      " [ 6  0  0]\n",
      " [21  0  0]]\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.1029 - acc: 0.3424 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.1029 - val_loss: 1.0979 - val_acc: 0.3438 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0979\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00045: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[21  0  0]\n",
      " [ 6  0  0]\n",
      " [25  0  0]]\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0954 - acc: 0.3843 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4442 - loss_no_weight_decay: 1.0954 - val_loss: 1.0975 - val_acc: 0.4038 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4442 - val_loss_no_weight_decay: 1.0975\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00046: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[27  0  0]\n",
      " [ 6  0  0]\n",
      " [19  0  0]]\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0580 - acc: 0.3765 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4442 - loss_no_weight_decay: 1.0580 - val_loss: 1.0974 - val_acc: 0.4219 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4442 - val_loss_no_weight_decay: 1.0974\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00047: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[25  0  0]\n",
      " [ 4  0  0]\n",
      " [23  0  0]]\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 21s 2s/step - loss: 1.1089 - acc: 0.3733 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.1089 - val_loss: 1.0978 - val_acc: 0.3654 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0978\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00048: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[30  0  0]\n",
      " [ 4  0  0]\n",
      " [18  0  0]]\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.0779 - acc: 0.3687 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.0779 - val_loss: 1.0979 - val_acc: 0.3438 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0979\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00049: val_kappa did not improve from 0.00000\n",
      "Confusion matrix: \n",
      " [[29  0  0]\n",
      " [ 7  0  0]\n",
      " [16  0  0]]\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 19s 2s/step - loss: 1.1240 - acc: 0.3703 - binary_accuracy: 0.6667 - mean_absolute_error: 0.4443 - loss_no_weight_decay: 1.1240 - val_loss: 1.0978 - val_acc: 0.3462 - val_binary_accuracy: 0.6667 - val_mean_absolute_error: 0.4443 - val_loss_no_weight_decay: 1.0978\n",
      "— val_kappa: 0.0 \n",
      "\n",
      "Epoch 00050: val_kappa did not improve from 0.00000\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Confusion matrix: \n",
      " [[28  0  0]\n",
      " [ 7  0  0]\n",
      " [17  0  0]]\n",
      "176/176 [==============================] - 5s 29ms/step\n",
      "176/176 [==============================] - 5s 28ms/step\n",
      "Test loss: 1.0975794914093884\n",
      "Test accuracy: 0.39204545454545453\n",
      "Confusion Matrix\n",
      "[[ 0  0 88]\n",
      " [ 0  0 19]\n",
      " [ 0  0 69]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        88\n",
      "           1       0.00      0.00      0.00        19\n",
      "           2       0.39      1.00      0.56        69\n",
      "\n",
      "    accuracy                           0.39       176\n",
      "   macro avg       0.13      0.33      0.19       176\n",
      "weighted avg       0.15      0.39      0.22       176\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "def get_experiment_id():\n",
    "    time_str = dt.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    experiment_id = 'base_{}'.format(time_str)\n",
    "    return experiment_id\n",
    "p_ratio=[1.0, 1.44, 1.73, 1.0]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Conv2D(16, (5, 5), activation='linear', input_shape=(300, 300, 3)))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Conv2D(16, (5, 5), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "#model.add(Dense(8,activation='tanh', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "#model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(FractionalPooling2D(pool_ratio=(1, 1.8, 1.8, 1),pseudo_random = True,overlap=True))\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Conv2D(16, (3, 3), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "#model.add(Dense(8,activation='tanh', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "#model.add(FractionalPooling2D(pool_ratio=(1, 1.8, 1.8, 1),pseudo_random = True,overlap=True))\n",
    "\n",
    "#model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "\n",
    "model.add(Conv2D(16, (5, 5), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Conv2D(16, (5, 5), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "#model.add(Dense(8,activation='tanh', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "#model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "\n",
    "model.add(FractionalPooling2D(pool_ratio=(1, 1.8, 1.8, 1),pseudo_random = True,overlap=True))\n",
    "\n",
    "model.add(Conv2D(16, (5, 5), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Conv2D(16, (5, 5), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "#model.add(InputLayer(input_tensor=tf.nn.fractional_max_pool(model.layers[1].output, p_ratio)[0]))\n",
    "#model.add(Dense(8,activation='tanh', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "#model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Conv2D(32, (3, 3), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "#model.add(Dense(8,activation='tanh', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "#model.add(LeakyReLU(alpha=0.01))          \n",
    "model.add(Conv2D(32, (3, 3), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Conv2D(32, (3, 3), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "#model.add(Dense(8,activation='tanh', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "#model.add(LeakyReLU(alpha=0.01))\n",
    "#model.add(FractionalPooling2D(pool_ratio=(1, 1.8, 1.8, 1),pseudo_random = True,overlap=True))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Conv2D(32, (3, 3), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "#model.add(Dense(8,activation='tanh', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(FractionalPooling2D(pool_ratio=(1, 1.7, 1.7, 1),pseudo_random = True,overlap=True))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Conv2D(32, (3, 3), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "#model.add(InputLayer(input_tensor=tf.nn.fractional_max_pool(model.layers[1].output, p_ratio)[0]))\n",
    "#model.add(FractionalPooling2D(pool_ratio=(1, 1.8, 1.8, 1),pseudo_random = True,overlap=True))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Conv2D(64, (3, 3), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(FractionalPooling2D(pool_ratio=(1, 1.7, 1.7, 1),pseudo_random = True,overlap=True))\n",
    "model.add(Conv2D(64, (3, 3), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Conv2D(64, (3, 3), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Conv2D(64, (3, 3), activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "#model.add(InputLayer(input_tensor=tf.nn.fractional_max_pool(model.layers[1].output, p_ratio)[0]))\n",
    "\n",
    "#model.add(MaxPooling2D(pool_size=1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(FractionalPooling2D(pool_ratio=(1, 1.7, 1.7, 1),pseudo_random = True,overlap=True))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "#chnaged from 10 to 5 because an error \n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.count_params()\n",
    "#adam = Adam(lr=0.0005, decay=1e-6)\n",
    "#rmsprop\n",
    "#Mloss = WeightedCategoricalCrossEntropy({0: 0.4125861396437394, 1: 1.4918664786083686, 2: 0.690426457789382, 3: 3.9541433021806855, 4: 4.885604311008468})\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['acc','binary_accuracy','mae'])\n",
    "model.summary()\n",
    "loss_no_weight_decay = model.total_loss - sum(model.losses)\n",
    "model.metrics_tensors.append(loss_no_weight_decay)\n",
    "model.metrics_names.append('loss_no_weight_decay')\n",
    "#############################################################################################\n",
    "#############################################################################################\n",
    "#############################################################################################\n",
    "#############################################################################################\n",
    "#Preparing Data Augmentation Configuration \n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "#        featurewise_center=True,\n",
    " #       samplewise_center=True,\n",
    "        shear_range=0,\n",
    "        zoom_range=0.0,#[1 / 1.15, 1.15],\n",
    "        horizontal_flip=False,#True,\n",
    "        rotation_range=0)#90,\n",
    "#        validation_split=0.2)\n",
    "\"\"\"\n",
    "For the test set only rescale should be made \n",
    "\"\"\"\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=r\"/media/husam/Data/ROP_EXp/three_class/train\",\n",
    "    target_size=(300, 300),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=64,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42)\n",
    "#    subset=\"training\")\n",
    "#    save_to_dir=\"D:/Separating Images into subfolders/train_augmented\")\n",
    "\n",
    "\n",
    "#print(train_generator.class_indices)\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    directory=r\"/media/husam/Data/ROP_EXp/three_class/vald\",\n",
    "    target_size=(300, 300),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=64,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42)\n",
    "#    subset=\"validation\")\n",
    "#    save_to_dir=\"D:/Separating Images into subfolders/val_augmented\"\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=r\"/media/husam/Data/ROP_EXp/three_class/test\",\n",
    "    target_size=(300, 300),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=1,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False,\n",
    "    seed=42)\n",
    "#    save_to_dir=\"D:/Separating Images into subfolders/test_augmented\")\n",
    "\n",
    "\n",
    "# Fitting/Training the model\n",
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "experiment_id = get_experiment_id()\n",
    "callbacks = [\n",
    "    QWK(valid_generator, STEP_SIZE_VALID),\n",
    "    EarlyStopping(monitor='val_acc', patience=50, min_delta=0.001, verbose=1),\n",
    "    ModelCheckpoint(experiment_id + \"-val_kapa_checkpoint.hdf5\", monitor='val_kappa', verbose=1, save_best_only=True, mode='max'),\n",
    "    ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=7, verbose=1, mode='auto', min_delta=0.001, cooldown=0, min_lr=1e-7),\n",
    "    VCM(valid_generator,STEP_SIZE_VALID)\n",
    "    \n",
    "]\n",
    "\n",
    "class_weight_list = class_weight.compute_class_weight('balanced', np.unique(train_generator.classes), train_generator.classes)\n",
    "class_weights = dict(zip(np.unique(train_generator.classes), class_weight_list))\n",
    "\n",
    "history=model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    class_weight=class_weights,\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=50  \n",
    ")\n",
    "\n",
    "\n",
    "# Evaluating the model \n",
    "#predicting output\n",
    "#reseting the test generator before calling predict_generator, in order not to get weird output\n",
    "test_generator.reset()\n",
    "#test_imgs, test_labels = next(test_generator)\n",
    "\n",
    "y_pred = model.predict_generator(test_generator, verbose=1,steps=len(test_generator))\n",
    "\n",
    "score = model.evaluate_generator(generator=test_generator, verbose=1,steps=len(test_generator))\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#y_pred =\n",
    "\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1) #multiple categories\n",
    "# mapping the labels to its classes \n",
    "labels = (train_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in y_pred]\n",
    "\n",
    "y_true = (test_generator.classes)\n",
    "#confusion Matrix and Classification Report\n",
    "# generator.classes gives you the truth label y_true\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "print('Classification Report')\n",
    "target_names = ['0', '1', '2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
